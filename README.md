# Implement-Gradient-Descent-for-Linear-Regression
To implement linear regression using Gradient Descent from scratch (without using scikit-learnâ€™s LinearRegression). Your task is to compare the closed-form solution (Normal Equation) with Gradient Descent on the same dataset.
ğŸš€ Linear Regression: Normal Equation vs Gradient Descent
ğŸ‘¨â€ğŸ“ Student Info
Name: Jilkapally Praneeth Teja

Student ID: [Your ID here]

Assignment: Implement Linear Regression using both the Normal Equation and Gradient Descent (without using scikit-learnâ€™s LinearRegression)

ğŸ“– Problem Statement
The objective of this assignment is to build Linear Regression from scratch using two approaches:

Closed-Form (Normal Equation)

Iterative (Gradient Descent)

Both models are trained on the same dataset to compare their parameter estimates, predictions, and performance.

ğŸ“Š Dataset
Generated 200 samples with:

ğ‘¦
=
3
+
4
ğ‘¥
+
ğœ–
whereÂ 
ğœ–
âˆ¼
ğ‘
(
0
,
1
)
y=3+4x+ÏµwhereÂ Ïµâˆ¼N(0,1)
ğ‘¥
âˆˆ
[
0
,
5
]
xâˆˆ[0,5], uniformly spaced.

Added Gaussian noise to simulate real-world data.

ğŸ› ï¸ Implementation
1ï¸âƒ£ Dataset Generation

Created 200 samples for 
ğ‘¥
x.

Added Gaussian noise.

Plotted the raw data points.

2ï¸âƒ£ Closed-Form Solution (Normal Equation)

Added a bias column of 1â€™s to 
ğ‘‹
X.

Computed parameters using:

ğœƒ =(ğ‘‹ğ‘‡
ğ‘‹
)
âˆ’
1
ğ‘‹
ğ‘‡
ğ‘¦
Î¸=(X 
T
 X) 
âˆ’1
 X 
T
 y
Printed intercept and slope.

Plotted the fitted line over the data.

3ï¸âƒ£ Gradient Descent

Initialized 
ğœƒ
=
[
0
,
0
]
Î¸=[0,0].

Learning rate: 
ğœ‚
=
0.05
Î·=0.05.

Iterations: 1000.

Tracked and plotted loss (MSE vs iterations).

Printed final intercept and slope.

4ï¸âƒ£ Comparison

Reported parameters from both approaches.

Verified that Gradient Descent converged to the same solution as the Normal Equation.

ğŸ“ˆ Results
Method	Intercept	Slope
Closed-Form	~3.01	~3.99
Gradient Descent	~3.01	~3.99

âœ… Both methods produced nearly identical results.

ğŸ“ Short Explanation
Both the Normal Equation and Gradient Descent minimized the same convex Mean Squared Error cost function, which is why they converged to almost the same parameters. The Normal Equation provides the exact solution in one step, while Gradient Descent iteratively approaches the same values â€” confirmed by the decreasing loss curve.
